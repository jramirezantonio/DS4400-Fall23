{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fce005f",
   "metadata": {},
   "source": [
    "## DS4400 Coding Exam\n",
    "\n",
    "This is the coding exam for DS4400. You have 100 minutes in the lecture. Please write down all the codes and in the Code chuck and written answers in the markdown chuck. Add any chuck if you need. Submit the exam as the homework, with both python and pdf file. The exam is open-book, open notes. Please raise your hand if you have any questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508c28c",
   "metadata": {},
   "source": [
    "#### Question 1: Data analysis\n",
    "In the following question, you will need to analysis a simulated data. Please answer each question below the instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8d3c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(4400)\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Create 20 random variables\n",
    "X = pd.DataFrame()\n",
    "for i in np.arange(1, 21):\n",
    "    variable_name = f\"Var{i}\"\n",
    "    X[variable_name] = np.random.rand(1000)\n",
    "\n",
    "# Create a target variable based on some combination of the 20 variables\n",
    "y = (\n",
    "    2 * X[\"Var1\"]\n",
    "    + 0.5 * X[\"Var5\"]\n",
    "    - 1 * X[\"Var10\"]\n",
    "    + np.random.normal(0, 0.5, num_samples)  # Add some noise\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ced033",
   "metadata": {},
   "source": [
    "1. Split the data into training and test data. The proportion of train data should be 70%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b11274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01306908",
   "metadata": {},
   "source": [
    "2. Fit the model with a linear regression using all the features, report the coefficient table, intercept and MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5171226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for this linear model is 0.2581677821622019\n",
      "The coefficient table is: \n",
      " 0     2.044796\n",
      "1    -0.065827\n",
      "2    -0.071376\n",
      "3     0.054124\n",
      "4     0.441769\n",
      "5     0.050972\n",
      "6    -0.057804\n",
      "7    -0.029059\n",
      "8    -0.131706\n",
      "9    -1.050051\n",
      "10    0.001533\n",
      "11    0.071195\n",
      "12    0.004165\n",
      "13   -0.023910\n",
      "14   -0.018469\n",
      "15   -0.065016\n",
      "16    0.014352\n",
      "17    0.084866\n",
      "18    0.079323\n",
      "19    0.072036\n",
      "dtype: float64\n",
      "The intercept coefficient is 0.057\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_linear = lm.predict(X_test)\n",
    "MSE_linear = mean_squared_error(y_test, y_linear)\n",
    "print(f\"The MSE for this linear model is {MSE_linear}\")\n",
    "\n",
    "coef_df = pd.Series(lm.coef_)\n",
    "print(\"The coefficient table is: \\n\", coef_df)\n",
    "\n",
    "print(\"The intercept coefficient is %.3f\" %lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359fcdb",
   "metadata": {},
   "source": [
    "3. Fit the model with a polynomial regression with degree 2, report the MSE. Is it necessary to use polynomial regression in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b96567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for this quadratic model is 0.4008640677627225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "deg = 2\n",
    "poly_features = PolynomialFeatures(degree=deg)\n",
    "\n",
    "X_poly_train = poly_features.fit_transform(X_train)\n",
    "X_poly_test = poly_features.fit_transform(X_test)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "\n",
    "y_poly_fit = poly_model.predict(X_poly_test)\n",
    "\n",
    "MSE_squared = mean_squared_error(y_test, y_poly_fit)\n",
    "print(f\"The MSE for this quadratic model is {MSE_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fcbb6",
   "metadata": {},
   "source": [
    "We observe the MSE for this model is higher than the MSE for the linear model. This should hint that a quadratic model is not a got fit for our data. Moreover, we see that our target variable y is defined as y = 2X_1 + 0.5X_5 - X_10 which is a linear equation, further indicating polynomial regression is not needed in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612c560",
   "metadata": {},
   "source": [
    "4. Fit the model with a Lasso regression, tune the parameter for the penalty parameter $\\alpha$. Report the best $\\alpha$, MSE and which variables are left in the model in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecdcabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuned alpha value is 0.01\n",
      "The MSE for a Lasso model is 0.251809272361641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "# Tuning alpha for Lasso\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {'alpha': alphas}\n",
    "\n",
    "# perform gridsearch\n",
    "lasso_model = Lasso()\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"The tuned alpha value is {best_alpha}\")\n",
    "\n",
    "# Optimized Lasso\n",
    "alpha = best_alpha\n",
    "\n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lasso = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "MSE_Lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"The MSE for a Lasso model is {MSE_Lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7033c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Features  Coefficient Estimates\n",
      "0      Var1               0.576660\n",
      "1      Var2              -0.009742\n",
      "2      Var3              -0.010318\n",
      "3      Var4               0.003577\n",
      "4      Var5               0.118299\n",
      "5      Var6               0.005104\n",
      "6      Var7              -0.003217\n",
      "7      Var8              -0.000000\n",
      "8      Var9              -0.030037\n",
      "9     Var10              -0.288913\n",
      "10    Var11               0.000000\n",
      "11    Var12               0.011492\n",
      "12    Var13              -0.000000\n",
      "13    Var14              -0.000000\n",
      "14    Var15              -0.000000\n",
      "15    Var16              -0.010187\n",
      "16    Var17              -0.000000\n",
      "17    Var18               0.010643\n",
      "18    Var19               0.009741\n",
      "19    Var20               0.011644\n"
     ]
    }
   ],
   "source": [
    "lasso_coef_df = pd.DataFrame(X.columns)\n",
    "lasso_coef_df.columns = ['Features']\n",
    "lasso_coef_df['Coefficient Estimates'] = pd.Series(lasso_model.coef_)\n",
    "coef_bool = lasso_coef_df['Coefficient Estimates'] != 0\n",
    "# lasso_coef_df['Coefficient Estimates'] = lasso_coef_df['Coefficient Estimates'][coef_bool]\n",
    "print(lasso_coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9009b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the variables except Var8, Var11, Var13, Var14, Var15, and Var17 are left at the end of model.\n"
     ]
    }
   ],
   "source": [
    "print(\"All the variables except Var8, Var11, Var13, Var14, Var15, and Var17 are left at the end of model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65e125",
   "metadata": {},
   "source": [
    "5. Define a new target variable $y_1$ such that $y_1$ only contains all the positive values in the $y$. Process $X$ as well. Fit the model with appropriate GLM model (not Gaussian). Report the MSE and can we compare the MSE with previous questions? Hint: it is a continous distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6553dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Gamma is:  0.14865729110851264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafid\\AppData\\Roaming\\Python\\Python310\\site-packages\\statsmodels\\genmod\\families\\links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_1 = y[y > 0]\n",
    "# print(len(y_1))\n",
    "X['y_1'] = y_1\n",
    "X.dropna(subset=['y_1'], inplace=True)\n",
    "# print(X)\n",
    "\n",
    "# Gamma model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y_1, test_size = 0.3)\n",
    "\n",
    "gamma_model = sm.GLM(y1_train, X_train, family = \n",
    "                     sm.families.Gamma(link = sm.families.links.log()))\n",
    "gamma_results = gamma_model.fit()\n",
    "y1_pred_gamma = gamma_results.predict(X_test)\n",
    "mse_gamma = mean_squared_error(y1_test, y1_pred_gamma)\n",
    "\n",
    "print(\"MSE for Gamma is: \", mse_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044da8c4",
   "metadata": {},
   "source": [
    "We choose the Gamma model for this distribution since our target value has continous positive values.\n",
    "We also observe that the MSE for this model is ~0.149 which is substantially smaller than our previous best model (Linear Regression) which had an MSE of ~0.258. This indicates the Gamma model is a better fit for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1b41b",
   "metadata": {},
   "source": [
    "6. Define a new target variable $y_2$ such that $y_2$ is a binary categorical variable. If $y$ is larger than 1, then $y_2$ is \"group1\", otherwise it is \"group2\". Fit the $y_2$ and $X$ with a logistic regression. Print the summary table with .summary(), and interpret the coefficient for variable 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2274db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Logistic Regression is:  0.03291886887997125\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  550\n",
      "Model:                            GLM   Df Residuals:                      529\n",
      "Model Family:                Binomial   Df Model:                           20\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -68.144\n",
      "Date:                Mon, 16 Oct 2023   Deviance:                       136.29\n",
      "Time:                        13:10:07   Pearson chi2:                     216.\n",
      "No. Iterations:                     9   Pseudo R-squ. (CS):             0.6792\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Var1          -1.2345      1.353     -0.913      0.361      -3.885       1.416\n",
      "Var2          -1.8641      0.828     -2.250      0.024      -3.488      -0.241\n",
      "Var3          -0.9241      0.833     -1.110      0.267      -2.556       0.708\n",
      "Var4          -1.5208      0.804     -1.891      0.059      -3.097       0.055\n",
      "Var5          -1.4012      0.829     -1.691      0.091      -3.025       0.223\n",
      "Var6          -1.5175      0.773     -1.963      0.050      -3.032      -0.003\n",
      "Var7          -0.1799      0.845     -0.213      0.831      -1.837       1.477\n",
      "Var8          -2.0046      0.843     -2.378      0.017      -3.657      -0.352\n",
      "Var9          -0.9540      0.825     -1.157      0.247      -2.570       0.662\n",
      "Var10         -1.2165      0.912     -1.334      0.182      -3.003       0.570\n",
      "Var11         -1.1378      0.849     -1.340      0.180      -2.802       0.526\n",
      "Var12         -2.5867      0.902     -2.866      0.004      -4.356      -0.818\n",
      "Var13         -0.1836      0.813     -0.226      0.821      -1.777       1.410\n",
      "Var14         -0.7472      0.861     -0.867      0.386      -2.435       0.941\n",
      "Var15         -1.9367      0.789     -2.454      0.014      -3.483      -0.390\n",
      "Var16         -1.3900      0.809     -1.719      0.086      -2.975       0.195\n",
      "Var17         -1.5836      0.829     -1.911      0.056      -3.208       0.041\n",
      "Var18          0.0757      0.828      0.091      0.927      -1.548       1.699\n",
      "Var19         -2.4758      0.835     -2.964      0.003      -4.113      -0.839\n",
      "Var20         -1.4108      0.834     -1.692      0.091      -3.045       0.223\n",
      "y_1           13.2114      1.742      7.584      0.000       9.797      16.626\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "y2 = (y_1 > 1).astype(int)\n",
    "# print(y_1)\n",
    "# print(y2)\n",
    "\n",
    "\n",
    "X_train, X_test, y2_train, y2_test = train_test_split(X, y2, test_size = 0.3)\n",
    "\n",
    "model = sm.GLM(y2_train, X_train, family = sm.families.Binomial())\n",
    "results = model.fit()\n",
    "y2_pred = results.predict(X_test)\n",
    "MSE = mean_squared_error(y2_test, y2_pred)\n",
    "print(\"MSE for Logistic Regression is: \",MSE)\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a295474",
   "metadata": {},
   "source": [
    "The Var1 coef. represents the change in the log-odds of y being larger than 1 for a one-unit change of Var1. Since the Var1 coef. is negative, it means that as Var1 increases, the log-odds of y being larger than 1 decrease. In simpler terms, an increase in Var1 is associated with a decrease in the probability of the event happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51d310",
   "metadata": {},
   "source": [
    "#### Question 2: Implement Gradient descend for Polynomial Regression\n",
    "\n",
    "Implement gradient descend method for the polynomial regression. Requirement: \n",
    "1. Write the method as a function, which is given here. Notice that it takes an input \"degree\" (and any other necessary inputs) so that we can change the degree of the polynomial. \n",
    "2. Output the cost history as well as the coefficient estimates. No need to print it or make the figures. As long as it is one of the output. \n",
    "3. Verify your function with the data in Question 1 (You may need to copy/paste and run the answer in Question 1-1 before you run the verification). No need to compare your coefficients to the ones in question 1. This step is only to make sure your functions work. You can set the degree as 2 in the verification. \n",
    "\n",
    "Hint: \n",
    "1. Don't overthinking the question. What is the difference between linear regression and polynomial regression?\n",
    "2. You may need two functions here. One for pre-processing the data, while the other one for gradient desent. You can add more if you need. Like to add another one for the cost function. \n",
    "3. When initializing the theta, think about how many coefficients you may need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1aa9662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def poly_function(degree, X):\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "    return X_poly\n",
    "\n",
    "def compute_cost(predictions, y):\n",
    "    cost = np.mean(np.square(predictions - y))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def poly_gradient_descent(X, y, num_iterations, learning_rate, degree):\n",
    "    n = len(y)\n",
    "    cost_history = np.zeros(num_iterations)\n",
    "    X_poly = poly_function(degree, X)\n",
    "    \n",
    "     # Initialize theta with zeros\n",
    "    total_features = X_poly.shape[1]\n",
    "    theta = np.zeros(total_features)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        predictions = X_poly.dot(theta)\n",
    "        errors = predictions - y\n",
    "        gradient = (2/n) * (errors.T.dot(X_poly))\n",
    "        theta -= learning_rate * gradient.T\n",
    "        cost_history[i] = compute_cost(predictions, y)\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41a02cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(4400)\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Create 20 random variables\n",
    "X = pd.DataFrame()\n",
    "for i in np.arange(1, 21):\n",
    "    variable_name = f\"Var{i}\"\n",
    "    X[variable_name] = np.random.rand(1000)\n",
    "\n",
    "# Create a target variable based on some combination of the 20 variables\n",
    "y = (\n",
    "    2 * X[\"Var1\"]\n",
    "    + 0.5 * X[\"Var5\"]\n",
    "    - 1 * X[\"Var10\"]\n",
    "    + np.random.normal(0, 0.5, num_samples)  # Add some noise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cc9e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final theta:\n",
      "[ 5.51012930e-02  2.19402395e-01  7.54772733e-03  4.39097686e-03\n",
      "  1.54345869e-02  5.99654282e-02  2.16342738e-02  2.14145978e-02\n",
      "  1.82570240e-02  1.11236336e-02 -9.27284254e-02  1.37268561e-02\n",
      "  2.56903398e-02  1.90417967e-02  1.92172197e-02  6.59892462e-03\n",
      "  5.81063274e-03  1.54610366e-03  1.94559797e-02  1.59776506e-02\n",
      "  2.43785499e-02  2.04630112e-01  1.02117620e-01  9.80080893e-02\n",
      "  1.10977194e-01  1.21967419e-01  1.11017522e-01  1.07218023e-01\n",
      "  1.05181820e-01  9.79264334e-02  5.45423069e-02  1.02988265e-01\n",
      "  1.03661012e-01  1.03377661e-01  1.03133234e-01  9.96068928e-02\n",
      "  9.86799275e-02  9.65423338e-02  1.07522489e-01  1.06283909e-01\n",
      "  1.08754406e-01 -6.82469641e-04 -6.62281829e-03 -5.21318809e-03\n",
      "  2.01186738e-02  2.97770210e-03 -2.80724870e-04 -4.38937943e-03\n",
      " -1.93188137e-05 -5.73304771e-02 -2.04836693e-03 -4.53715223e-03\n",
      "  1.65117346e-03 -5.38199766e-03 -6.47407617e-03 -1.07233501e-02\n",
      " -5.28876122e-03  3.80770659e-04 -4.97053086e-03  1.28335941e-03\n",
      " -7.93874049e-03 -3.72558771e-03  1.49740707e-02  2.26855693e-03\n",
      "  7.54521129e-03 -7.40632943e-03 -4.26341179e-03 -5.14915566e-02\n",
      " -2.24350142e-03  2.23391467e-03  2.93767583e-04  4.28131075e-03\n",
      " -9.06980185e-03 -1.07617544e-02 -9.88770736e-03  2.93513568e-03\n",
      "  3.96370561e-04  1.00404179e-03  5.39090454e-03  2.08874680e-02\n",
      " -2.78397319e-03  6.84450285e-04 -8.82087143e-04 -5.09852049e-03\n",
      " -5.68777943e-02 -2.28038527e-03 -1.62164893e-03  7.34387468e-03\n",
      "  8.11357957e-03 -3.23601694e-03  1.40927110e-03  8.11307103e-04\n",
      "  4.86129443e-04  5.74778719e-03  9.45780738e-03  4.74003504e-02\n",
      "  2.96324267e-02  2.33913292e-02  2.71982255e-02  2.33620306e-02\n",
      " -2.24641485e-02  2.29292486e-02  2.59675734e-02  2.50834845e-02\n",
      "  2.65593355e-02  1.89541396e-02  1.79488318e-02  1.79195985e-02\n",
      "  2.55496395e-02  2.20108183e-02  2.61442334e-02  1.53709420e-02\n",
      "  4.84104515e-03  1.12560190e-02 -7.25566921e-04 -4.98355602e-02\n",
      "  1.05922646e-02  7.80042220e-03  6.19742015e-03  3.94842372e-03\n",
      "  2.35843412e-03 -7.29885828e-04 -5.42174854e-03  1.00462020e-02\n",
      "  4.29112817e-03  5.88847438e-03  1.56289050e-02  9.50387906e-03\n",
      "  1.23219442e-03 -5.24043620e-02  5.88475612e-03  1.46754896e-02\n",
      "  1.22205983e-02  5.03830796e-03 -1.02665816e-03  5.69323758e-04\n",
      "  2.26491066e-03  3.98642453e-03  8.03264992e-03  6.80289021e-03\n",
      "  5.50191626e-03  5.66729262e-04 -5.38535826e-02 -2.39796452e-03\n",
      "  1.67776409e-03  3.12655735e-03 -1.85367949e-03 -3.97493844e-03\n",
      "  2.24664831e-03 -1.21095142e-02  7.52067353e-03 -1.36467439e-03\n",
      "  8.23569686e-03  2.21995441e-03 -5.79946416e-02 -1.24588453e-03\n",
      "  2.57484240e-03 -3.08975210e-04  8.82988932e-03 -8.23766325e-03\n",
      " -5.75442363e-03 -9.93823453e-03 -6.68436898e-04 -6.26666984e-03\n",
      "  6.24163248e-03 -1.01056760e-01 -4.65760321e-02 -4.57290790e-02\n",
      " -5.13275742e-02 -5.02504598e-02 -6.10788004e-02 -5.60947217e-02\n",
      " -5.99274739e-02 -5.22106697e-02 -5.46880886e-02 -4.39748266e-02\n",
      "  4.20777953e-05 -2.42160629e-03  7.49456028e-04  1.43316326e-02\n",
      " -2.64288674e-03 -4.54942649e-03 -1.11688501e-02  8.05288733e-03\n",
      "  1.57450891e-03  2.94485631e-03  1.93660432e-02  8.24211008e-03\n",
      "  2.59651396e-03  4.63844631e-03 -1.97597322e-03 -5.09216864e-03\n",
      "  1.06073089e-02  6.88161020e-03  1.16054469e-02  9.37888008e-03\n",
      "  7.45263336e-03 -3.23355832e-03 -3.17488002e-03 -3.12314824e-03\n",
      "  2.42941301e-04  5.63219183e-03  1.43850617e-02  8.02032580e-04\n",
      "  5.08456218e-03 -3.40081915e-03  1.96127311e-03  1.13949798e-02\n",
      "  4.47582496e-03  6.75727468e-03 -3.62938401e-05 -1.05646660e-02\n",
      " -9.89052717e-03 -5.84062708e-03  5.47563993e-03  2.12545230e-03\n",
      " -1.14564223e-02 -1.18325080e-02  1.04197204e-03 -5.83374884e-03\n",
      "  2.59780082e-04 -1.06197707e-02 -3.30101589e-03 -2.82309904e-03\n",
      "  9.05715913e-03  1.10806866e-02  2.58071707e-03  8.63030000e-03\n",
      "  3.67963553e-03  3.89601062e-03  1.06592640e-02]\n",
      "Cost History:\n",
      "[1.22403441 0.86801996 0.74341071 0.69576765 0.67381696 0.66053359\n",
      " 0.65026242 0.64112299 0.63248802 0.62414635 0.61602499 0.60809691\n",
      " 0.60035041 0.59277893 0.58537767 0.57814249 0.57106952 0.56415504\n",
      " 0.55739544 0.55078719 0.54432685 0.53801106 0.53183652 0.52580004\n",
      " 0.51989848 0.51412878 0.50848793 0.50297303 0.4975812  0.49230967\n",
      " 0.48715569 0.48211662 0.47718984 0.47237281 0.46766304 0.46305811\n",
      " 0.45855564 0.45415332 0.44984888 0.4456401  0.44152482 0.43750094\n",
      " 0.43356637 0.42971912 0.4259572  0.42227868 0.4186817  0.41516439\n",
      " 0.41172498 0.4083617  0.40507283 0.4018567  0.39871167 0.39563614\n",
      " 0.39262854 0.38968734 0.38681105 0.38399821 0.38124739 0.3785572\n",
      " 0.37592626 0.37335326 0.37083689 0.36837588 0.36596898 0.36361498\n",
      " 0.3613127  0.35906096 0.35685865 0.35470464 0.35259786 0.35053726\n",
      " 0.34852178 0.34655043 0.34462222 0.34273617 0.34089136 0.33908685\n",
      " 0.33732174 0.33559515 0.33390622 0.33225412 0.33063801 0.32905709\n",
      " 0.32751058 0.32599772 0.32451774 0.32306991 0.32165353 0.32026788\n",
      " 0.31891229 0.31758608 0.3162886  0.3150192  0.31377728 0.3125622\n",
      " 0.31137338 0.31021023 0.30907219 0.30795869]\n"
     ]
    }
   ],
   "source": [
    "degree = 2\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "theta_final, cost_history = poly_gradient_descent(X, y, num_iterations, learning_rate, degree)\n",
    "\n",
    "# Print the final parameters (theta) after training\n",
    "print(\"Final theta:\")\n",
    "print(theta_final)\n",
    "\n",
    "print(\"Cost History:\")\n",
    "print(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b738e7",
   "metadata": {},
   "source": [
    "#### Question 3: Simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af4348",
   "metadata": {},
   "source": [
    "Following is a simulation study. In the second code chuck, please correctly label the xlabel and ylabel for the plot. Also explain what this code is trying to do and what you have learned from the generated figure. \n",
    "\n",
    "Hint: in the simulation data, there are 200 observations and it is fixed for each trial. After spliting the training and testing data, each one will have 100 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddef77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(83)\n",
    "A = np.arange(5,101,1)\n",
    "B = []\n",
    "\n",
    "for p in A:\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "    for i in np.arange(1, p+1):\n",
    "        variable_name = f\"Var{i}\"\n",
    "        X[variable_name] = np.random.rand(200)\n",
    "\n",
    "    y = 2 * X[\"Var1\"] - 0.5 * X[\"Var5\"] + np.random.normal(0, 0.5, 200)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.5, \n",
    "                                                        random_state=83)\n",
    "    \n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "    y_pred = lm.predict(X_test)\n",
    "    value = mean_squared_error(y_test, y_pred)\n",
    "    B.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4654def2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(A, B)\n",
    "plt.xlabel(\"Number of Predictor Variables\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11911b9e",
   "metadata": {},
   "source": [
    "For each value of 'p' in A (range from 5 to 100 inclusive), a DataFrame 'X' is created with 'p' random predictor variables ('Var1', 'Var2', ..., 'Varp'). The response variable 'y' is generated as a linear combination of 'Var1' and 'Var5' plus some random noise.\n",
    "\n",
    "The data is then split into training and testing sets with 50% test data and 50% train data. A linear regression model is trained using the training set, and a prediction is obtained using the linear model for the testing set. Lastly, the MSE is computed for each value of 'p'.\n",
    "\n",
    "We then plot the MSE values for each 'p' against the number of predictor variables ('Var1', 'Var2', ..., 'Varp').\n",
    "\n",
    "We see that after a certain number of predictor variables, our MSE skyrockets indicating that more predictor variables are not always better. There's a relationship between model complexity (number of features) and prediction accuracy.\n",
    "For this data in particular, we observe lower MSE's when the number of predictor variables is less than 20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
